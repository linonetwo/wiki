created: 20230421021437399
modified: 20230421073208747
tags: 任务 让太微更加智能
title: 调研GPU上运行的语言模型
tmo_taskState: New
type: text/vnd.tiddlywiki

* ChatRwkv直接用 CPU 或 GPU 就能运行
** 需要改代码里的模型地址，例如 `+args.MODEL_NAME = ' C:/model/LanguageModel/RWKV/RWKV-4-Raven-14B-v9-Eng99%-Other1%-20230412-ctx8192.pth'`
** 运行 14B 模型时有 40 层，需要增加 i8 量化的层数（启动时会第一行展示模型有几层），使用 `args.strategy = 'cuda fp16i8 *30 -> cuda fp16'` 表示前 30 层 i8 量化，后 10 层正常 fp16 运行
** 将 `os.environ["RWKV_CUDA_ON"] = '1'` 设为 1 才会使用 CUDA，不然仅仅是使用了 GPU
*** 报错 `Command '['where', 'cl']' returned non-zero exit status 1` ，需要安装 Visual Studio 2022，然后把安装文件夹里包含 cl.exe 的文件加到系统 path 里，例如 `E:\ProgramFiles\VisualStudio\VC\Tools\MSVC\14.35.32215\bin\Hostx64\x64` ，然后在开始菜单里搜环境变量打开配置工具，在里面新加一行，然后注销系统账号重新登录即可
*** 报错 `Could not find module 'C:\Users\Administrator\AppData\Local\torch_extensions\torch_extensions\Cache\py310_cu118\wkv_cuda\wkv_cuda.pyd'`，通过使用 stable difussion webui 的 venv 解决，应该是需要安装 pytorch
**** 报错 `Error building extension 'wkv_cuda'`，看 issue 说明不能用 venv，需要使用系统安装的 python 环境
*** 通过重新安装 RWKV 推荐版本的 CUDA11.7.1 和相应的 pytorch 解决，[ext[安装方法在issue里|https://github.com/BlinkDL/ChatRWKV/issues/99#issuecomment-1517335586]]
** 24G 显存的 RTX3090ti 可以用 `'cuda fp16i8 *20 -> cuda fp16'` 策略运行，每多 5 层 fp16，大约多占用 2GB 显存。`cuda fp16i8 *30 -> cuda fp16` 则占用 18.7 GB， `*25` 占用 20GB


